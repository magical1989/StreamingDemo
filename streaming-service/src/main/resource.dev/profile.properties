spark.master=local[*]
spark.app.name=spark.app.name
spark.streaming.concurrentJobs=4
spark.default.parallelism=10
spark.executor.extraJavaOptions=-XX:NewRatio=1 -XX:SurvivorRatio=8 -XX:+UseCompressedOops -XX:SoftRefLRUPolicyMSPerMB=0 -XX:MaxPermSize=512m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/apps/logs/spark/
spark.kryoserializer.buffer.mb=20
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.speculation=false
spark.storage.memoryFraction=0.3
spark.executor.logs.rolling.strategy=time
spark.executor.logs.rolling.time.interval=daily
spark.executor.logs.rolling.maxRetainedFiles=3

kmp.spark.streaming.duration=1
kmp.spark.streaming.window_duration=60
kmp.spark.streaming.slide_duration=1

kafka.zookeeper_host=spmaster:2187,spslave1:2187,spslave2:2187,spslave3:2187,spslave4:2187
kafka.group=kafka.group
kafka.topics=kafka.topic

offline.debug=debug
offline.debug.hadoop.home.dir=E:/Java/winutils
offline.debug.socket.ip=127.0.0.1
offline.debug.socket.port=8333
offline.debug.checkpoint.dir=

checkpoint.dir=

elasticsearch.clustername=
elasticsearch.hosts=