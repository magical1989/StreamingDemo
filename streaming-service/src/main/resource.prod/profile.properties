spark.app.name=spark.app.name
spark.streaming.concurrentJobs=4
spark.default.parallelism=10
spark.executor.extraJavaOptions=-Xmn3072m -XX:+UseCompressedOops -XX:SoftRefLRUPolicyMSPerMB=0 -XX:MaxPermSize=512m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/logs/train/logs/ -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSParallelRemarkEnabled -XX:+CMSConcurrentMTEnabled -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintFlagsFinal
spark.kryoserializer.buffer.mb=20
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.speculation=false
spark.storage.memoryFraction=0.3
spark.executor.logs.rolling.strategy=time
spark.executor.logs.rolling.time.interval=daily
spark.executor.logs.rolling.maxRetainedFiles=3
spark.rdd.compress=true
#不立即停止,完成当前批次数据后再关闭
spark.streaming.stopGracefullyOnShutdown=true

kmp.spark.streaming.duration=2
kmp.spark.streaming.window_duration=7200
kmp.spark.streaming.slide_duration=2

kafka.zookeeper_host=spmaster:2187,spslave1:2187,spslave2:2187,spslave3:2187,spslave4:2187
kafka.group=kafka.group
kafka.topics=kafka.topic

#debug,no
offline.debug=no
offline.debug.checkpoint.dir=

checkpoint.dir=/tmp/spark.app.name

elasticsearch.clustername=
elasticsearch.hosts=